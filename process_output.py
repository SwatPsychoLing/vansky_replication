##Taking the OUTPUT files (file?) generated by the neural net
##adding necessary information for running through R
  # specifically stimulus conditions ('ambiguous', 'unambiguous', 'filler')
##then following below (running through pandas)
  #load each list into pandas,
  #use python function to identify the disambiguating regions based on sentence position
 ##NOTE: uses pandas
 ##MISSING CRITICAL ITEM NUM 70 from A4 Reversed

##################################################################################################
## NOTE: To get proper heading in output result file, need to run ListA1.adapting as first list ##

 ########################################
 ## EXAMPLE FUNCTION CALL:
 ##python process_output.py ListA1.adapting finejaeger.csv
 #######################################

import argparse
import pandas as pd

###### USER DEFINED: DIRECTORY OF ORIGINAL AND RESULTS FILES
directory_og = 'fj-original/'   ##directory of the original sentence files - complete w conditions
directory_res = 'fj-try3-output/'  ##directory of the processed output files
results_file = ''
original_file = ''

def main():
    global results_file, original_file
    ##setting up expected command-line arguments
    parser = argparse.ArgumentParser()
    parser.add_argument('list_name', type=lambda x:validate_list(parser, x))
    parser.add_argument('processed_filename')  ##where the processed output will be saved
    args = parser.parse_args()

    ##go through both files and put into pandas dataframe
    ##---ORIGINAL FILE---##
    #use original file to generate dictionary mapping from sentence num to:
        # condition, sentid (__)
    sent_original_mappings = {}
    ##for each line in the 'original file' - aka file with conditions n such
    for ol in original_file.readlines():
        curr_l = ol.strip("\n").split(",")  ##turn line into list
        ##if line containing a sentence - aka first value is a num
        if(curr_l[0].isnumeric()):
            ##add line num - 1 to mappings (since results starts indexing at 0 instead of 1)
            line_num = str(int(curr_l[0])-1)
            ##get condition - either 'filler', 'ambig', or 'unambig'
            if curr_l[3] == '':
                condition = 'filler'
            elif curr_l[3] == '1':
                condition = 'ambig'
            else:
                condition = 'unambig'
            #get sentid
            sentid = curr_l[5][1:]  ##want to cut off letter C/F and just have num
            ##assign dict value: key = line num, val = {condition: condition, sentid: sentid}
            sent_original_mappings[line_num] = {'condition':condition, 'sentid':sentid}

    ##---RESULTS FILE---##
    ##get columns of data fram from heading of results file
    columns = results_file.readline().strip("\n").split(" ")
    ##get the data from the results file
    line_reslst = []  ##list containing all results from file
        #each index corresponds to a word presented in that order
    condition_list = []  ##list keeping track of condition values for each sentence
        #each index corresponds to word presented at that num, contains condition of sentence
    intended_sentid = 0   ##counter to keep track of proper sentence id (since output produced weird format)
    prev_listedsentid = 0
    #for each line in the results file
    for l in results_file.readlines():
        if(l[0] == '='):  ##reached end of data
            break
        else:  ##otherwise add next data point to list of lines
            l_listified = l.strip("\n").split(" ")  #turn line into list
            ##reformatted files have odd numbering system
            #so manually keeping track of proper sentence id
            if(int(l_listified[1]) > prev_listedsentid):   #if crrent sentence is next in line
                intended_sentid += 1  ##reached next sent - increment counter
                prev_listedsentid = int(l_listified[1])
            ##add condition value to list  - based off mapping from sentid
            condition_list.append(sent_original_mappings[str(intended_sentid)]['condition'])
            ##add overall result line to line_reslst, inputting proper sent id rather than incorrect one
            line_reslst.append(l_listified[0:1] + [str(intended_sentid)] + l_listified[2:])

    data = pd.DataFrame(line_reslst, columns=columns)  #create dataframe of results list
    data['condition'] = condition_list  ##add condition column to dataframe

    ##then modify pandas dataframe as replication did
    ##NOTE: BELOW CODE IS COPIED (and modified) FROM VANSKY REPLICATION ##
    data_region = data
    data_region['region'] = data_region.apply(lambda row: add_regionnums(row),axis=1)
    data_region=data_region[data_region['region']==3]
    data_region=data_region[data_region['condition']!='filler'].reset_index().drop('index',axis=1)
    data_region=data_region.groupby(('sentid','condition')).agg('mean').reset_index()
    try:
        data_region['order']=range(40)
    except:  ##weird result override: if a random cirtical item is missing...
        num = -1
        for i in range(39,0,-1):
            try:
                num = i
                data_region['order']=range(i)
                break
            except:
                pass
        print("found critical: %d" % num)  ##print the number of critical items found
    #######################################################
    ##then write out to processed file in csv format
    inf = open(args.processed_filename, 'a')
    ##write lists in csv format
    ##WANT HEADINGS: list, sentid (critical), condition, order, word, sentpos, region (3/0), surp
    heading = "list,sentid,condition,order,word,sentpos,region,surp\n"   ##get proper heading from column names
    list = args.list_name.split(".")[0]
    if(args.list_name == "ListA1.adapting"):  ##only write heading for first list; NOTE: hardcoded!
        file_output = heading  ##concatenate all info to write into file at end
    else:
        file_output = ""
    ##go through rows in data_region dataframe -- aka iterate through critical sentences
    for row_num, row in data_region.iterrows():
        curr_list_sentid = row['sentid']
        true_sentid = sent_original_mappings[curr_list_sentid]['sentid']
        condition = sent_original_mappings[curr_list_sentid]['condition']
        order = row['order']
        region = row['region']
        ##for each critical sentence, look up and iterate through words from results file
        all_word_res = data.loc[data['sentid']==curr_list_sentid]
        ##for each word in the sentence
        for word_row_num, word_row in all_word_res.iterrows():
            if(word_row['region'] == 3):  ##ignore any words not in disambiguating region  -- MY MOD
                word = word_row['word']
                sentpos = word_row['sentpos']
                surp = word_row['surp']
                ##write according to heading
                file_output += str(list) +","+ str(true_sentid)+"," + str(condition) +"," + str(order) + "," + str(word) + "," + str(sentpos) + "," + str(region) + "," + str(surp) +"\n"

    inf.write(file_output)
    ##close files
    inf.close()
    results_file.close()
    original_file.close()


def validate_list(parser, listname):
    """
    given parser and name of current list
    opens and returns the results and original file for list
    using directories global vars
    """
    if (valid_ogfile(listname) and valid_resfile(listname)):
        return listname
    else:
        parser.error("Filename error: %s" % listname)

def valid_ogfile(listname):
    """
    given name of file
    assigns global original file (opened)
    returns false if unable to open file
    """
    global original_file, directory_og
    listname = listname.split(".")[0]  #get rid of adapting or notadapting
    try:
        original_file = open(directory_og+listname+".csv", "r")
        return True
    except:
        return False

def valid_resfile(listname):
    """
    given name of list
    assigns global results file (opened)
    returns false if unable to open file
    """
    global results_file, directory_res
    try:
        results_file = open(directory_res+listname+".output", "r")
        return True
    except:
        return False


###################################################################
# BELOW FUNCTION IS PROVIDED FROM VANSKY REPLICATION INSTRUCTIONS #
###################################################################
def add_regionnums(row):
    if row['condition'] == 'ambig' and row['sentpos'] in ('7','8','9'):
        #ambiguous critical region starts with word 7
        return(3)
    elif row['condition'] == 'unambig' and row['sentpos'] in ('9','10','11'):
        #unambiguous critical region starts with word 9
        return(3)
    else:
        return(0)

#######
main()
